{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdaqf6vgTTg4"
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsClassification_evaluation_security_dataset_L227201964-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2VgPk2mTThF"
   },
   "source": [
    "# **Action classification task based on Internet Firewall logs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkRBtAnQTThK"
   },
   "source": [
    "The purpose of this lab is to build the best classifier in the security area based on Internet Firewall DataSet with the helping frameworks & libraries.\n",
    "\n",
    "After completing this lab you will\n",
    "\n",
    "1.  Be able to explore the Internet Firewall DataSet and build the best classifier via selecting from a set of existed classifiers.\n",
    "2.  Be able to show different calculated metrics of existed classifiers.\n",
    "3.  Be able to visualize the data analysis results with various plot types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHwOXFUpTThK"
   },
   "source": [
    "## Agenda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHSc4iksTThP"
   },
   "source": [
    "*   Theory and Methods\n",
    "*   General part\n",
    "    *   Import required libraries and dataset\n",
    "    *   Explore the Dataset\n",
    "    *   Form a set of classifiers and fit them\n",
    "    *   Metrics calculation and choice better classifier\n",
    "    *   Visualization of results\n",
    "*   Tasks\n",
    "*   Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4t5rpwITThQ"
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNAvKmglBm-"
   },
   "source": [
    "## Theory and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOgyr5RSlI40"
   },
   "source": [
    "The data that we are going to use for this is a subset of an open source default of Internet Firewall DataSet from the UCI ML repository: [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsClassification_evaluation_security_dataset_L227201964-2022-01-01).\n",
    "\n",
    "> This dataset is public available for research. The details are described in \\[Fatih Ertam, fatih.ertam '@' firat.edu.tr, Firat University, Turkey].\n",
    "\n",
    "Please include this citation if you plan to use this database:\n",
    "F. Ertam and M. Kaya, Classification of firewall log files with multiclass support vector machine, in 6th International Symposium on Digital Forensic and Security, ISDFS 2018 - Proceeding, 2018.\n",
    "\n",
    "During the work, you will know about the [classification task](https://en.wikipedia.org/wiki/Statistical_classification?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsClassification_evaluation_security_dataset_L227201964-2022-01-01#Algorithms) is the main machine learning task solved by an algorithm correlating some input data with one or more classes, which must be defined earlier. In essence, classification can be used as a tool for many other tasks, such as automatic language detection, 3D-reconstruction, etc.\n",
    "\n",
    "Moreover, you will be provided `11` well-known classifiers. You should select a better classifier among them. Some of them need enough more time for learning by `.fit` method, so you can select not all classifiers, but, for instance, 5 or 6 will be enough.\n",
    "\n",
    "Classification refers to a supervised learning strategy, also called supervised or guided learning.\n",
    "\n",
    "A classification task is often referred to as predicting a categorical dependent variable (i.e. a dependent variable that is a category) based on a sample of continuous and / or categorical variables.\n",
    "\n",
    "In addition, we will build the visualization of our results, exactly the obtained metrics (accuracy and loss) to choose a better classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS8UOiQOTThR"
   },
   "source": [
    "## Import required libraries and dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6YFQWnQHKui"
   },
   "source": [
    "Download data using a URL and rename it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bN5NPujBTThV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-22 07:20:04--  https://archive.ics.uci.edu/ml/machine-learning-databases/00542/log2.csv\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252, 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘log2.csv’\n",
      "\n",
      "log2.csv                [   <=>              ]   2.74M  5.10MB/s    in 0.5s    \n",
      "\n",
      "2025-02-22 07:20:05 (5.10 MB/s) - ‘log2.csv’ saved [2876998]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00542/log2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BnB2tiSZh4R"
   },
   "source": [
    "Alternative URL for downloading of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ORfD1EUiZuLK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-22 07:20:05--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Classification_evaluation_security_dataset_L2/log2.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2811465 (2.7M) [text/csv]\n",
      "Saving to: ‘log2.csv.1’\n",
      "\n",
      "log2.csv.1          100%[===================>]   2.68M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-02-22 07:20:05 (64.7 MB/s) - ‘log2.csv.1’ saved [2811465/2811465]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Classification_evaluation_security_dataset_L2/log2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGH5DhdLHmOp"
   },
   "source": [
    "Import needed libraries to use in this lab. We can add some aliases (such as pd, plt, np, sns) to make the libraries easier to use in code and set a default figure size for further plots. Ignore warnings too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OK6PuHMwS1Ed",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-plot\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-plot) (3.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-plot) (0.20.1)\n",
      "Requirement already satisfied: scipy>=0.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-plot) (1.7.3)\n",
      "Collecting joblib>=0.10 (from scikit-plot)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (8.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "Installing collected packages: joblib, scikit-plot\n",
      "Successfully installed joblib-1.3.2 scikit-plot-0.3.7\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Collecting scikit-learn>=1.0.2 (from imbalanced-learn->imblearn)\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from imbalanced-learn->imblearn)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.20.1\n",
      "    Uninstalling scikit-learn-0.20.1:\n",
      "      Successfully uninstalled scikit-learn-0.20.1\n",
      "Successfully installed imbalanced-learn-0.12.4 imblearn-0.0 scikit-learn-1.0.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jdVcijxmTTha",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scikitplot as sk\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import *\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.style.use('ggplot')\n",
    "seed = 42\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evtbog-C13Z2"
   },
   "source": [
    "As well as specify the value of the `precision` parameter equal to 3 to display three decimal signs (instead 6 as default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nwMoaJLV13Z3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"precision\", 3)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmrZguMTThd"
   },
   "source": [
    "## Explore the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvMGO3m4TThd"
   },
   "source": [
    "In this section you will explore the sourse dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ-GKs1J13Zz"
   },
   "source": [
    "Let's read the data and look at the first 5 rows using the `head` method. The count of the output rows from the dataset is determined by the `head` method parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wamAf3HLm_FS",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>NAT Source Port</th>\n",
       "      <th>NAT Destination Port</th>\n",
       "      <th>Action</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Bytes Sent</th>\n",
       "      <th>Bytes Received</th>\n",
       "      <th>Packets</th>\n",
       "      <th>Elapsed Time (sec)</th>\n",
       "      <th>pkts_sent</th>\n",
       "      <th>pkts_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57222</td>\n",
       "      <td>53</td>\n",
       "      <td>54587</td>\n",
       "      <td>53</td>\n",
       "      <td>allow</td>\n",
       "      <td>177</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>4768</td>\n",
       "      <td>1600</td>\n",
       "      <td>3168</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6881</td>\n",
       "      <td>50321</td>\n",
       "      <td>43265</td>\n",
       "      <td>50321</td>\n",
       "      <td>allow</td>\n",
       "      <td>238</td>\n",
       "      <td>118</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>3327</td>\n",
       "      <td>1438</td>\n",
       "      <td>1889</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>443</td>\n",
       "      <td>45848</td>\n",
       "      <td>443</td>\n",
       "      <td>allow</td>\n",
       "      <td>25358</td>\n",
       "      <td>6778</td>\n",
       "      <td>18580</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source Port  Destination Port  NAT Source Port  NAT Destination Port  \\\n",
       "0        57222                53            54587                    53   \n",
       "1        56258              3389            56258                  3389   \n",
       "2         6881             50321            43265                 50321   \n",
       "3        50553              3389            50553                  3389   \n",
       "4        50002               443            45848                   443   \n",
       "\n",
       "  Action  Bytes  Bytes Sent  Bytes Received  Packets  Elapsed Time (sec)  \\\n",
       "0  allow    177          94              83        2                  30   \n",
       "1  allow   4768        1600            3168       19                  17   \n",
       "2  allow    238         118             120        2                1199   \n",
       "3  allow   3327        1438            1889       15                  17   \n",
       "4  allow  25358        6778           18580       31                  16   \n",
       "\n",
       "   pkts_sent  pkts_received  \n",
       "0          1              1  \n",
       "1         10              9  \n",
       "2          1              1  \n",
       "3          8              7  \n",
       "4         13             18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('log2.csv', header = 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjzxXtUp13Z3"
   },
   "source": [
    "### Let's look at the dataset size, feature names and their types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "53xc7fWwoOMD",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65532, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtO5KNTV13Z5"
   },
   "source": [
    "The dataset contains `65 532` objects (rows), for each of which `12` features are set (columns), including 1 target feature (`Action`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVOmrfNH3pRs"
   },
   "source": [
    "### Attribute Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSTqzHlbKWoa"
   },
   "source": [
    "Output the column (feature) names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3uuAnHe3nvCh",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source Port', 'Destination Port', 'NAT Source Port',\n",
       "       'NAT Destination Port', 'Action', 'Bytes', 'Bytes Sent',\n",
       "       'Bytes Received', 'Packets', 'Elapsed Time (sec)', 'pkts_sent',\n",
       "       'pkts_received'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SDpd1dxKeq4"
   },
   "source": [
    "Input features (column names):\n",
    "\n",
    "1.  `Source Port` - a port, from which data transfer has been carried out\n",
    "2.  `Destination Port` - a port, to which data transfer has been carried out\n",
    "3.  `NAT Source Port` - a port, which needs to track for outgoing traffic from NAT (Network Address Translation) side to retransfer transit packets\n",
    "4.  `NAT Destination Port` - a port, which needs to track for incoming traffic from NAT (Network Address Translation) side to retransfer transit packets\n",
    "5.  `Action` - a feature (desired target), which is used as a class, 4 classes in total (`allow`, `deny`, `drop`, and `reset-both`)\n",
    "6.  `Bytes` - bytes total, is equal to the sum of the next two columns\n",
    "7.  `Bytes Sent` - the amount of data sent in bytes\n",
    "8.  `Bytes Received` - the amount of data received in bytes\n",
    "9.  `Packets` – packets total, is equal to the sum of the last two columns\n",
    "10. `Elapsed Time (sec)` - time taken to transfer data in seconds\n",
    "11. `pkts_sent` - the amount of data sent in packets\n",
    "12. `pkts_received` - the amount of data received in packets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6SaYJgj13Z6"
   },
   "source": [
    "To see general information on the all dataframe features (columns), we use the **`info`** method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LCV1-dAJ13Z6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65532 entries, 0 to 65531\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Source Port           65532 non-null  int64 \n",
      " 1   Destination Port      65532 non-null  int64 \n",
      " 2   NAT Source Port       65532 non-null  int64 \n",
      " 3   NAT Destination Port  65532 non-null  int64 \n",
      " 4   Action                65532 non-null  object\n",
      " 5   Bytes                 65532 non-null  int64 \n",
      " 6   Bytes Sent            65532 non-null  int64 \n",
      " 7   Bytes Received        65532 non-null  int64 \n",
      " 8   Packets               65532 non-null  int64 \n",
      " 9   Elapsed Time (sec)    65532 non-null  int64 \n",
      " 10  pkts_sent             65532 non-null  int64 \n",
      " 11  pkts_received         65532 non-null  int64 \n",
      "dtypes: int64(11), object(1)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic2B0eP0PshN"
   },
   "source": [
    "As you can see, the dataset is full, no pass (`non-null`), so there is no need to fill the skippings. The dataset contains 11 integer (`int64`) and 1 object (`object`) features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2I_fcSH13Z8"
   },
   "source": [
    "Method **`describe`** shows the main statistical characteristics of the dataset for each numerical feature (`int64` type): existing values count, mean, standard deviation, range, min & max, 0.25, 0.5 and 0.75 quartiles. Will show the obtained results in transposed view for better perception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JGuHLQMl13Z8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Source Port</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>49391.969</td>\n",
       "      <td>15255.713</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49183.000</td>\n",
       "      <td>53776.500</td>\n",
       "      <td>58638.000</td>\n",
       "      <td>65534.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destination Port</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>10577.386</td>\n",
       "      <td>18466.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>445.000</td>\n",
       "      <td>15000.000</td>\n",
       "      <td>65535.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAT Source Port</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>19282.973</td>\n",
       "      <td>21970.690</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8820.500</td>\n",
       "      <td>38366.250</td>\n",
       "      <td>65535.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAT Destination Port</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>2671.050</td>\n",
       "      <td>9739.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>443.000</td>\n",
       "      <td>65535.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bytes</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>97123.950</td>\n",
       "      <td>5618438.909</td>\n",
       "      <td>60.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>168.000</td>\n",
       "      <td>752.250</td>\n",
       "      <td>1269359015.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bytes Sent</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>22385.797</td>\n",
       "      <td>3828138.650</td>\n",
       "      <td>60.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>210.000</td>\n",
       "      <td>948477220.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bytes Received</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>74738.153</td>\n",
       "      <td>2463207.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>449.000</td>\n",
       "      <td>320881795.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packets</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>102.866</td>\n",
       "      <td>5133.002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1036116.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elapsed Time (sec)</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>65.834</td>\n",
       "      <td>302.462</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>10824.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pkts_sent</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>41.400</td>\n",
       "      <td>3218.871</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>747520.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pkts_received</th>\n",
       "      <td>65532.000</td>\n",
       "      <td>61.467</td>\n",
       "      <td>2223.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>327208.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count      mean         std    min       25%  \\\n",
       "Source Port          65532.000 49391.969   15255.713  0.000 49183.000   \n",
       "Destination Port     65532.000 10577.386   18466.027  0.000    80.000   \n",
       "NAT Source Port      65532.000 19282.973   21970.690  0.000     0.000   \n",
       "NAT Destination Port 65532.000  2671.050    9739.162  0.000     0.000   \n",
       "Bytes                65532.000 97123.950 5618438.909 60.000    66.000   \n",
       "Bytes Sent           65532.000 22385.797 3828138.650 60.000    66.000   \n",
       "Bytes Received       65532.000 74738.153 2463207.712  0.000     0.000   \n",
       "Packets              65532.000   102.866    5133.002  1.000     1.000   \n",
       "Elapsed Time (sec)   65532.000    65.834     302.462  0.000     0.000   \n",
       "pkts_sent            65532.000    41.400    3218.871  1.000     1.000   \n",
       "pkts_received        65532.000    61.467    2223.332  0.000     0.000   \n",
       "\n",
       "                           50%       75%            max  \n",
       "Source Port          53776.500 58638.000      65534.000  \n",
       "Destination Port       445.000 15000.000      65535.000  \n",
       "NAT Source Port       8820.500 38366.250      65535.000  \n",
       "NAT Destination Port    53.000   443.000      65535.000  \n",
       "Bytes                  168.000   752.250 1269359015.000  \n",
       "Bytes Sent              90.000   210.000  948477220.000  \n",
       "Bytes Received          79.000   449.000  320881795.000  \n",
       "Packets                  2.000     6.000    1036116.000  \n",
       "Elapsed Time (sec)      15.000    30.000      10824.000  \n",
       "pkts_sent                1.000     3.000     747520.000  \n",
       "pkts_received            1.000     2.000     327208.000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz-xyMpdVC3I"
   },
   "source": [
    "The `Mean` row shows the feature average,`  STD ` is a RMS (Root Mean Square) deviation, `min`,`  max ` - minimum and maximum values, `25%`, `50%`, ` 75%  `- quarters that split the dataset (or part of it) into four groups containing approximately an equal number of observations (rows). For example, the average elapsed time for data transfer (`Elapsed Time (sec)`) is a bit more than one minute (`65.834` seconds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUuf9K8JQ6-c"
   },
   "source": [
    "In general, according to the data, it is impossible to say that there are [outliers](https://en.wikipedia.org/wiki/Outlier?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsClassification_evaluation_security_dataset_L227201964-2022-01-01) in the data. However, such an inspection is not enough, it is desirable to still see the charts of the dependence of the target feature from each feature, but you can do it later yourself, when you will visualize features and dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0-sAGXl13Z_"
   },
   "source": [
    "Because we have only one object (type `object`) feature, let check and show the unique values of its.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pjchGbdx13aA",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['allow', 'drop', 'deny', 'reset-both'], dtype=object)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df[i].unique() for i in df if df[i].dtypes == \"object\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7tj4stSK5D_"
   },
   "source": [
    "As you can see the feature (desired target) really has 4 classes (`allow`, `drop`, `deny`, `reset-both`), as it has shown earlier.\n",
    "\n",
    "The next our step is to convert to lowercase all column names and change gaps between words to sign `_` for better perception and for the convenience of further work with the Internet Firewall DataSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lyrfDQcMQGj-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>nat_source_port</th>\n",
       "      <th>nat_destination_port</th>\n",
       "      <th>action</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_sent</th>\n",
       "      <th>bytes_received</th>\n",
       "      <th>packets</th>\n",
       "      <th>elapsed_time_(sec)</th>\n",
       "      <th>pkts_sent</th>\n",
       "      <th>pkts_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57222</td>\n",
       "      <td>53</td>\n",
       "      <td>54587</td>\n",
       "      <td>53</td>\n",
       "      <td>allow</td>\n",
       "      <td>177</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>4768</td>\n",
       "      <td>1600</td>\n",
       "      <td>3168</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6881</td>\n",
       "      <td>50321</td>\n",
       "      <td>43265</td>\n",
       "      <td>50321</td>\n",
       "      <td>allow</td>\n",
       "      <td>238</td>\n",
       "      <td>118</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>3327</td>\n",
       "      <td>1438</td>\n",
       "      <td>1889</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>443</td>\n",
       "      <td>45848</td>\n",
       "      <td>443</td>\n",
       "      <td>allow</td>\n",
       "      <td>25358</td>\n",
       "      <td>6778</td>\n",
       "      <td>18580</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51465</td>\n",
       "      <td>443</td>\n",
       "      <td>39975</td>\n",
       "      <td>443</td>\n",
       "      <td>allow</td>\n",
       "      <td>3961</td>\n",
       "      <td>1595</td>\n",
       "      <td>2366</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60513</td>\n",
       "      <td>47094</td>\n",
       "      <td>45469</td>\n",
       "      <td>47094</td>\n",
       "      <td>allow</td>\n",
       "      <td>320</td>\n",
       "      <td>140</td>\n",
       "      <td>180</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_port  destination_port  nat_source_port  nat_destination_port  \\\n",
       "0        57222                53            54587                    53   \n",
       "1        56258              3389            56258                  3389   \n",
       "2         6881             50321            43265                 50321   \n",
       "3        50553              3389            50553                  3389   \n",
       "4        50002               443            45848                   443   \n",
       "5        51465               443            39975                   443   \n",
       "6        60513             47094            45469                 47094   \n",
       "\n",
       "  action  bytes  bytes_sent  bytes_received  packets  elapsed_time_(sec)  \\\n",
       "0  allow    177          94              83        2                  30   \n",
       "1  allow   4768        1600            3168       19                  17   \n",
       "2  allow    238         118             120        2                1199   \n",
       "3  allow   3327        1438            1889       15                  17   \n",
       "4  allow  25358        6778           18580       31                  16   \n",
       "5  allow   3961        1595            2366       21                  16   \n",
       "6  allow    320         140             180        6                   7   \n",
       "\n",
       "   pkts_sent  pkts_received  \n",
       "0          1              1  \n",
       "1         10              9  \n",
       "2          1              1  \n",
       "3          8              7  \n",
       "4         13             18  \n",
       "5         12              9  \n",
       "6          3              3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(\" \",\"_\")\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iGbs3lUMQZ3"
   },
   "source": [
    "Let's check the balancing of the feature (desired target). The following function `Counter` from the `collections` framework can be used for this action as more convenient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GzIE-4dBQPEg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'allow': 37640, 'drop': 12851, 'deny': 14987, 'reset-both': 54})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VMc-H8KNC89"
   },
   "source": [
    "As you can see, our feature (desired target) is imbalanced (amount `allow` > `deny` > `drop` > `reset-both`), but we will solve this issue.\n",
    "\n",
    "The following actions (encoding, dropping, cutting by quantiles range, selecting of required \"clean\" subset, resampling, splitting to train and test sets, and scaling) help you to prepare your dataset for usage for the classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "63dQAaURQZUl",
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (_metadata_requests.py, line 1512)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3552\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_421/1784721138.py\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from imblearn.over_sampling import SMOTE\n",
      "  File \u001b[1;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/imblearn/__init__.py\"\u001b[0m, line \u001b[1;32m52\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from . import (\n",
      "  File \u001b[1;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/imblearn/ensemble/__init__.py\"\u001b[0m, line \u001b[1;32m6\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from ._bagging import BalancedBaggingClassifier\n",
      "  File \u001b[1;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/imblearn/ensemble/_bagging.py\"\u001b[0m, line \u001b[1;32m30\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from ..pipeline import Pipeline\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/imblearn/pipeline.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .utils._metadata_requests import (\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/imblearn/utils/_metadata_requests.py\"\u001b[0;36m, line \u001b[0;32m1512\u001b[0m\n\u001b[0;31m    def process_routing(_obj, _method, /, **kwargs):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# to encode our feature (desired target)\n",
    "lab = preprocessing.LabelEncoder()\n",
    "df.action = lab.fit_transform(df.action)\n",
    "\n",
    "# create new dataframe without our feature (desired target)\n",
    "data_out = df.drop(\"action\", axis = 1)\n",
    "\n",
    "# define 1st and 3rd quantiles, interquartile range and outlier step (1.5 interquartile range) \n",
    "# form a needed range for a selecting required \"clean\" subset without outliers accordance to Tukey method\n",
    "quant_1 = data_out.quantile(0.25)\n",
    "quant_3 = data_out.quantile(0.75)\n",
    "diff = quant_3 - quant_1\n",
    "data = df[~((df < (quant_1 - 1.5 * diff)) |(df > (quant_3 + 1.5 * diff))).any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hZUA0RtT9Zs"
   },
   "source": [
    "Indicate the shape size of the new dataset & count the observations amount of each class.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vOwPIutT9lC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFfUF_UWh7"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "data.shape, Counter(data.action)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5qD0fkNVB6b"
   },
   "source": [
    "In addition, will show a histogram distribution of the observations amount of each class. Let's use the SMOTE function to make the classes in balanced. The SMOTE function does the oversampling on the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heqLYeSTRCXP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = sns.countplot(data.action)\n",
    "for p in fig.patches:\n",
    "    fig.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), size = 12,\n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf11M_orV8dq"
   },
   "source": [
    "Will select `feature` and `target` variables, based on which you will split our new dataset into subsets for future training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWe8cL2cRKTW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature, target = data.drop(\"action\", axis = 1).values, data[\"action\"].values\n",
    "sm = SMOTE(random_state = seed)\n",
    "feature, target = sm.fit_resample(feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHLDRO1KWdxy"
   },
   "source": [
    "After using SMOTE function (i.e. the over-sampling method) all classes are same and good to make prediction on it. This method also allows to avoid over-fitting issues. Let's check the observations amount of each class and answer (based on `target` variable) the question \"Is our dataset balanced now?\"\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwDnh9T8W12c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrxbyL-W3Bt"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "Counter(target)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_jX2RlkXLUB"
   },
   "source": [
    "Let's split our dataset with a proportion: test set = `30%`, train set = `70%` (these values can be chosen depends on task and some other factors). Also, will use `shuffle` function of the samples to have always a random order of samples fed to the network. Set the seed to a constant value so that the random reordering is reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfgjZ8woSXaP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = model_selection.train_test_split(feature, target,\n",
    "                                                                test_size = 0.3,\n",
    "                                                                random_state = seed,\n",
    "                                                                stratify = target,\n",
    "                                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lU8BsUQaSgb"
   },
   "source": [
    "Indicate the shape size of all split datasets.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0nZeIBEadqh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcpeVlUoadyS"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "xtrain.shape, xtest.shape, ytrain.shape, ytest.shape\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-KI7QPwscA1"
   },
   "source": [
    "Further, we need to make scaling. Method `StandardScaler` transforms your data (train and test sets) in such a way that its distribution will have a mean of 0 and a standard deviation of 1. Thus this method normalizes or standardizes your data individually, independently for each column. This is useful when you want to compare data for different measurement units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_da1RoqDSfOP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain = preprocessing.StandardScaler().fit_transform(xtrain)\n",
    "xtest = preprocessing.StandardScaler().fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAcfAfOGFlcV"
   },
   "source": [
    "## Form a set of classifiers and fit them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcbIuofyvLDK"
   },
   "source": [
    "Will describe `11` well-known classifiers with some minimal parameters. Some of them are commented out, so you should uncomment by rows only those, which you want to study. Thus, you can select not all classifiers, but, for instance, `5` or `6` will be enough.\n",
    "\n",
    "Besides, the approximate execution time is shown at the right in a row with classifier description. If you will obtain the following message \"Warning: Variables are collinear\", then never mind, because this Lab has not an aim to eliminate this warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trtqWSjpSjEI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = os.cpu_count()\n",
    "Classifiers = {\n",
    "    \"Linear_Regressor\": linear_model.LogisticRegression(C = 10, solver = 'liblinear'),  ## duration is less than 1 min\n",
    "    # \"SVC\": svm.SVC(random_state = seed),  ## duration is about 10 min\n",
    "    # \"Linear_SVC\": svm.LinearSVC(max_iter = 10000, random_state = seed),  ## duration is about 4 min\n",
    "    \"Random_Forest\": ensemble.RandomForestClassifier(random_state = seed, n_jobs = AUTOTUNE),  ## duration is about 1-2 min\n",
    "    \"Ada_Boost\": ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(random_state = seed),\n",
    "                                             random_state = seed,\n",
    "                                             learning_rate = 0.1),  ## duration is about 2 min\n",
    "    # \"Gradient_Boost\": ensemble.GradientBoostingClassifier(random_state = seed),  ## duration is about 10 min\n",
    "    \"Ex_Tree_Classifier\": ensemble.ExtraTreesClassifier(random_state = seed, n_jobs = AUTOTUNE),  ## duration is about 1-2 min\n",
    "    # \"ML_PC\": neural_network.MLPClassifier(max_iter = 1000, random_state = seed),  ## duration is about 20 min\n",
    "    \"kNN\": neighbors.KNeighborsClassifier(n_jobs = AUTOTUNE),  ## duration is about 2-3 min\n",
    "#     \"QDA\": discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = np.finfo(float).eps),  ## duration is less than 1 min\n",
    "    \"Decision_Tree\": tree.DecisionTreeClassifier(random_state = seed)  ## duration is less than 1 min\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqPAnUlkxbEc"
   },
   "source": [
    "Let's visualize the learning process. You should use the `plot_learning_curve` function, which cross-validates all of your data. Let set the parameter `cv` = `kfold`, it means stratified 10-fold strategy of splitting cross-validations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTfmkmZgSn7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "kfold = StratifiedKFold(n_splits = 10)\n",
    "for name, classifier in Classifiers.items():\n",
    "    sk.estimators.plot_learning_curve(classifier, xtrain, ytrain,\n",
    "                                      cv = kfold,\n",
    "                                      title = name,\n",
    "                                      random_state = seed,\n",
    "                                      n_jobs = -1)\n",
    "print(\"Elapsed time:\", round(time.time() - start_time, 2), 'sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzVJ-1rOxjPa"
   },
   "source": [
    "The graphs visualize the `learning curves` of the chosen models for both training and test (validation) sets as the size of the training set increases. The shaded area of the learning curve represents the uncertainty of that curve (measured as `standard deviation`). The model is evaluated on both training and test sets using the coefficient of determination `R2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeFikQo0JhpR"
   },
   "source": [
    "## Metrics calculation and choice better classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8D2DVgcMNAw"
   },
   "source": [
    "Let's teach our set of classifiers (which you selected earlier) wuth the help of `fit` method. After each fitting stage, we will calculate some metrics, which helps us to chose the best classifier for the further optimization. We will use the following metrics: `train & test accuracy`, `log_loss`, and numbers of `correct` and `incorrect` cases of a predict from our models (classifiers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNEWFAxmUgAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "start_time = time.time()\n",
    "logging_cols=[\"Classifier\", \"Test accuracy\", \"Train accuracy\", \"Log_loss\", \"Correct_cases\", \"Incorrect_cases\"]\n",
    "logging = pd.DataFrame(columns = logging_cols)\n",
    "\n",
    "for name_c , classif_c in Classifiers.items():\n",
    "    classif_c.fit(xtrain, ytrain)\n",
    "    predict = classif_c.predict(xtest)\n",
    "    accuracy = classif_c.score(xtest, ytest)\n",
    "    train_pred = classif_c.predict_proba(xtest)\n",
    "    lg_ls = log_loss(ytest, train_pred)\n",
    "    train_test_acc = classif_c.score(xtrain, ytrain)\n",
    "    correct = (ytest == predict).sum()\n",
    "    incorrect = (ytest != predict).sum()\n",
    "    \n",
    "    print(\"=\"*45)\n",
    "    print('****    Estimations    ****')\n",
    "    print(f\"Test accuracy:  {accuracy*100:.2f}%    Name: {name_c}\")\n",
    "    print(f\"Train accuracy:  {train_test_acc*100:.2f}%\")\n",
    "    print(f\"Log_loss:  {lg_ls:.4f}\")\n",
    "    print(f\"Correct   points:  {correct}\")\n",
    "    print(f\"Incorrect points:  {incorrect}\")\n",
    "    sk.metrics.plot_confusion_matrix(ytest, predict, title = name_c, cmap = plt.cm.Greens)\n",
    "    logging_entry = pd.DataFrame([[name_c, accuracy*100, train_test_acc*100, lg_ls, correct, incorrect]], columns = logging_cols)\n",
    "    logging = logging.append(logging_entry)\n",
    "print(\"\\nElapsed time:\", round(time.time() - start_time, 2), 'sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAiACgwDQs_G"
   },
   "source": [
    "You should show the DataFrame with all calculated metrics.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_vVp-p8eypC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMWqWXCcRFCW"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "logging\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFOR5VRoJobq"
   },
   "source": [
    "## Visualization of results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQtqCnFKRQSc"
   },
   "source": [
    "Let's visualize calculated metrics with the same classifiers that to get a chance to choose the best classifier. These visualizations can help us to understand classifiers achievements and compare them more clearly. We will do this for the `accuracy` metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh5-IzqktzLW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "graph = sns.barplot(x = 'Classifier',\n",
    "                    y = 'Test accuracy',\n",
    "                    data = logging,          \n",
    "                    palette = 'hls')\n",
    "\n",
    "for p in graph.patches:\n",
    "    graph.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, -12), size = 12,\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.xlabel('\\nClassifier`s name')\n",
    "plt.title('Estimations (Test accuracy) of the classifiers')\n",
    "plt.show()\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "graph = sns.barplot(x = 'Classifier',\n",
    "                    y = 'Train accuracy',\n",
    "                    data = logging,          \n",
    "                    palette = 'hls')\n",
    "\n",
    "for p in graph.patches:\n",
    "    graph.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, -12), size = 12,\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.xlabel('\\nClassifier`s name')\n",
    "plt.title('Estimations (Train accuracy) of the classifiers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f01ykq1UV3u8"
   },
   "source": [
    "After selection of the best or a set of the best classifiers, you can go to the next stage is hyperparameters optimization, but it is a topic of the further Lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMjKczqTThq"
   },
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLCT0HXkTThr"
   },
   "source": [
    "You should build the same visualizations, but only for the `Log_loss`, `Correct_cases`, `Incorrect_cases` metrics.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIFR1-sLoKmJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWKD4lEOmLEo"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "sns.set_color_codes(\"muted\")\n",
    "graph = sns.barplot(x = 'Classifier',\n",
    "                    y = 'Log_loss',\n",
    "                    data = logging,          \n",
    "                    palette = 'hls')\n",
    "\n",
    "for p in graph.patches:\n",
    "    graph.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, -12), size = 12,\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.xlabel('\\nClassifier`s name')\n",
    "plt.title('Estimations (Log_loss) of the classifiers')\n",
    "plt.show()\n",
    "\n",
    "graph = sns.barplot(x = 'Classifier',\n",
    "                    y = 'Correct_cases',\n",
    "                    data = logging,          \n",
    "                    palette = 'hls')\n",
    "\n",
    "for p in graph.patches:\n",
    "    graph.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, -12), size = 12,\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.xlabel('\\nClassifier`s name')\n",
    "plt.title('Estimations (Correct_cases) of the classifiers')\n",
    "plt.show()\n",
    "\n",
    "graph = sns.barplot(x = 'Classifier',\n",
    "                    y = 'Incorrect_cases',\n",
    "                    data = logging,          \n",
    "                    palette = 'hls')\n",
    "\n",
    "for p in graph.patches:\n",
    "    graph.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, -12), size = 12,\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.xlabel('\\nClassifier`s name')\n",
    "plt.title('Estimations (Incorrect_cases) of the classifiers')\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEbNXiqmTTh0"
   },
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_1ZOvXKTTh0"
   },
   "source": [
    "[Sergii Kavun](https://www.linkedin.com/in/sergii-kavun/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsClassification_evaluation_security_dataset_L227201964-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHpM3GyTTTh2"
   },
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrgMUa_ETTh2"
   },
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description   |\n",
    "| ----------------- | ------- | ------------- | -------------------- |\n",
    "| 2021-07-08        | 1.0     | Kavun, Sergii | Code improving       |\n",
    "| 2021-06-07        | 0.21    | Kavun, Sergii | Code refactoring     |\n",
    "| 2021-06-02        | 0.2     | Kavun, Sergii | Translate to english |\n",
    "| 2021-06-01        | 0.1     | Kavun, Sergii | Created Lab          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma5EMufCTTh2"
   },
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification_evaluation_dataset_L2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
